<!DOCTYPE html><html lang="null"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"/><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"/><meta content="yes" name="apple-mobile-web-app-capable"/><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"/><meta content="telephone=no" name="format-detection"/><meta name="description"/><title>Scrapy做一点微小的事情 | Sumarry and Promotion</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"/><link rel="stylesheet" type="text/css" href="/css/pure-min.css"/><link rel="stylesheet" type="text/css" href="/css/grids-responsive-min.css"/><link rel="stylesheet" type="text/css" href="/css/style.css"/><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"/><link rel="apple-touch-icon" href="/apple-touch-icon.png"/><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"/><link rel="alternate" type="application/atom+xml" href="/atom.xml"/></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Scrapy做一点微小的事情</h1><a id="logo" href="">Sumarry and Promotion</a><p class="description">Peaceful Thinking with LGrok</p></div><div id="nav-menu"><a href="/" class="current"><i class="icon-home"> 首頁</i></a><a href="/archives/"><i class="icon-archive"> 所有文章</i></a><a href="/about/"><i class="icon-about"> 關於</i></a><a href="/atom.xml"><i class="icon-rss"> 訂閱</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post post-page"><h1 class="post-title">Scrapy做一点微小的事情</h1><div class="post-meta">2016-09-27 | </div><span data-thread-key="2016/09/27/Scrapy-Babys/" class="ds-thread-count"></span><div class="post-content"><h2 id="0x01__u722C_u866B"><a href="#0x01__u722C_u866B" class="headerlink" title="0x01 爬虫"></a>0x01 爬虫</h2><p>爬虫是一种自动化获取网络信息的程序。Scrapy是python实现的著名的爬虫框架。以前实现过一些简单的爬虫项目，用以获取特定信息。我做的爬虫基于两个模块，一是目标解析获取、二是站点页面解析，通过解析初始页面向消费者队列中添加需要爬取得url，页面解析模块则从队列中拉取url解析并存储数据。这是最基本的爬虫模型，看起来可扩展性差了很多。最近没有什么开发项目，便熟悉下scrapy框架，学习优秀的产品，做低调的魔法师。</p>
<h2 id="0x02_Scrapy_Tutorial"><a href="#0x02_Scrapy_Tutorial" class="headerlink" title="0x02 Scrapy Tutorial"></a>0x02 Scrapy Tutorial</h2><p>最初接触Scrapy，觉得思路还是特别凌乱。其指南中介绍了简单的使用方法。<br>首先可以使用scrapy命令行中的<code>scrapy startproject [project name]</code>创建你自己的scrapy项目。scrapy会自动为你生成类似下面的目录结构。<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">tutorial/</span><br><span class="line">    scrapy.cfg            <span class="comment"># deploy configuration file</span></span><br><span class="line"></span><br><span class="line">    tutorial/             <span class="comment"># project's Python module, you'll import your code from here</span></span><br><span class="line">        __init__.py</span><br><span class="line"></span><br><span class="line">        <span class="keyword">items</span>.py          <span class="comment"># project items file</span></span><br><span class="line"></span><br><span class="line">        pipelines.py      <span class="comment"># project pipelines file</span></span><br><span class="line"></span><br><span class="line">        settings.py       <span class="comment"># project settings file</span></span><br><span class="line"></span><br><span class="line">        spiders/          <span class="comment"># a directory where you'll later put your spiders</span></span><br><span class="line">            __init__.py</span><br><span class="line">            ...</span><br></pre></td></tr></table></figure></p>
<p>在spider目录中，你可以编写自己的爬虫模块，例如dmoz_spider.py:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DmozSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"dmoz"</span>						<span class="comment"># 名字是每个爬虫必须的，scrapy框架会根据你选择的爬虫名字寻找要启动的项目</span></span><br><span class="line">    allowed_domains = [<span class="string">"dmoz.org"</span>]</span><br><span class="line">    start_urls = [						<span class="comment">#这里是爬虫的起点，定义了起始爬取url</span></span><br><span class="line">        <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/"</span>,</span><br><span class="line">        <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span>			<span class="comment">#默认处理访问start_urls后返回内容的函数</span></span><br><span class="line">        filename = response.url.split(<span class="string">"/"</span>)[-<span class="number">2</span>] + <span class="string">'.html'</span></span><br><span class="line">        <span class="keyword">with</span> open(filename, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.body)</span><br></pre></td></tr></table></figure></p>
<p>然后，第一个爬虫就做好了，最初可以使用scrapy命令行中<code>scrapy crawl [spider name]</code>也就是<code>scrapy crawl dmoz</code>运行第一个爬虫，他会讲访问start_urls之后的内容解析，并将内容写入到一个文件中。<br>当然这并不能满足我日常工作的需求，检索scrapy的文档，通篇思路顺序有些混乱，记录下自己的思路，是放下寂寞。</p>
<h2 id="0x03_Deepen_Scrapy"><a href="#0x03_Deepen_Scrapy" class="headerlink" title="0x03 Deepen Scrapy"></a>0x03 Deepen Scrapy</h2><p>官方给出的框架图是这个样子的：<br><img src="/images/scrapy_architecture.png" alt="scrapy_architecture"><br>1.<strong>Scrapy Engine</strong>是用来控制数据流向，触发特定事件；<br>2.<strong>Scheduler</strong>中存放着需要访问的目标urls，更像是生产者与消费者中的消费队列；<br>3.<strong>Downloader</strong>从Scheduler中提取url并访问目标页面，返回访问结果；<br>4.<strong>Spider</strong>接受Downloader返回的结果，解析后返回Item或者一个新的访问请求；<br>5.<strong>Item Pipeline</strong>数据输出管道，接受spider返回的Item，对其做适当处理。</p>
<p>整体数据流向是，Engine处理最初的访问请求，并使用parse方法将需要的请求提交到schedule，Downloader定时从Scheduler中取出目标进行访问，期间数据流入Downloader Middlewares，在下载器中间件可以根据request的情况处理访问头（使用<code>process_request(request, spider)</code>方法）、甚至直接返回一个<code>response</code>对象，跳过Downloader直接流出下载器中间件，执行<code>process_response(request,response,spider)</code>方法，至此产出的response流到Spider Middlewares中间件，这个中间件主要对返回的结果进行搜集、解析并进行相应的处理。期间数据流经Spider产生最后的结果或者新的requests。<br>需要特别注意的是，每个中间件都有流入流出两个方向，处理着不同的访问需求，细分如下：</p>
<ul>
<li>下载器流入：通常用于处理请求头的修改；</li>
<li>下载器流出：通常用于修改返回结果、检查返回状态码重新提交请求；</li>
<li>爬虫流入：通常用于检查返回状态，记录并抛出异常；</li>
<li>爬虫流出：通常用于解析完成后的异常处理，可以返回response、request或者异常。</li>
</ul>
<h2 id="0x04_Demo"><a href="#0x04_Demo" class="headerlink" title="0x04 Demo"></a>0x04 Demo</h2><p>使用Scrapy实现的<a href="https://github.com/LinLL/scrapy_pic" target="_blank" rel="external">煎蛋爬虫</a>，一个随机代理随机UA的爬虫。<br>详细教程以后再写吧</p>
</div><div class="tags"><a href="/tags/Python/">Python</a><a href="/tags/spider/">spider</a></div><div class="post-nav"><a href="/2016/09/04/blinker/" class="next">Blinker Information Module<i class="icon-next"></i></a></div><div data-thread-key="2016/09/27/Scrapy-Babys/" data-title="Scrapy做一点微小的事情" data-url="http://LinLL.github.io/2016/09/27/Scrapy-Babys/" class="ds-share flat"><div class="ds-share-inline"><ul class="ds-share-icons-16"><li data-toggle="ds-share-icons-more"><a href="javascript:void(0);" class="ds-more">分享到：</a></li><li><a href="javascript:void(0);" data-service="weibo" class="ds-weibo">微博</a></li><li><a href="javascript:void(0);" data-service="qzone" class="ds-qzone">QQ空间</a></li><li><a href="javascript:void(0);" data-service="qqt" class="ds-qqt">腾讯微博</a></li><li><a href="javascript:void(0);" data-service="wechat" class="ds-wechat">微信</a></li></ul><div class="ds-share-icons-more"></div></div></div><div data-thread-key="2016/09/27/Scrapy-Babys/" data-title="Scrapy做一点微小的事情" data-url="http://LinLL.github.io/2016/09/27/Scrapy-Babys/" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1-4"><div id="sidebar"><div class="widget"><div class="widget-title">分類</div></div><div class="widget"><div class="widget-title">標籤</div><div class="tagcloud"><a href="/tags/uwsgi/" style="font-size: 15px;">uwsgi</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/distribute/" style="font-size: 15px;">distribute</a> <a href="/tags/spider/" style="font-size: 15px;">spider</a> <a href="/tags/db/" style="font-size: 15px;">db</a> <a href="/tags/nginx/" style="font-size: 15px;">nginx</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/php/" style="font-size: 15px;">php</a> <a href="/tags/RCE/" style="font-size: 15px;">RCE</a> <a href="/tags/write/" style="font-size: 15px;">write</a> <a href="/tags/nodejs/" style="font-size: 15px;">nodejs</a> <a href="/tags/web/" style="font-size: 15px;">web</a> <a href="/tags/vul/" style="font-size: 15px;">vul</a></div></div><div class="widget"><div class="widget-title">最新文章</div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2016/09/27/Scrapy-Babys/">Scrapy做一点微小的事情</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/04/blinker/">Blinker Information Module</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/05/05/ImageMagick-RCE-Dili-CMS/">ImageMagick RCE && Dili CMS</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/04/11/Nginx-Flask-uWSGI-deploy/">Nginx+Flask+uWSGI deploy</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/04/08/DiliCMS-vulnerabilities/">DiliCMS vulnerabilities</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/03/26/Celery-Daemon-Init/">Celery Daemon Init</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/02/29/Beautiful-Doc-Sphinx/">Beautiful Doc --Sphinx</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/02/25/First-use-Flask/">First use Flask</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/02/21/SQLAlchemy/">SQLAlchemy</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/02/18/Simple-Celery/">Simple Celery</a></li></ul></div><div class="widget"><div class="comments-title">最近評論</div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title">友站連結</div></div></div></div></div><div id="footer">© <a href="/" rel="nofollow">Sumarry and Promotion.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div><a id="rocket" href="#top" class="show"></a><script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/totop.js" type="text/javascript"></script><script src="/js/fancybox.pack.js" type="text/javascript"></script>
<script src="/js/jquery.fancybox.js" type="text/javascript"></script><link rel="stylesheet" href="/css/jquery.fancybox.css" type="text/css"><script>var duoshuoQuery = {short_name:'lgrok'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-74395323-1','auto');ga('send','pageview');</script></div></body></html>